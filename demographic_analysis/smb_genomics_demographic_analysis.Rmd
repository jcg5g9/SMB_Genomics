---
title: 'Analysis 7: Demographic analysis'
author: "Joe Gunn"
date: "2022-09-05"
output: html_document
---

# Project: Population genomic analysis of Smallmouth Bass and Neosho Bass in the Central Interior Highlands
We investigated the extent of genomic divergence, local directional selection, and admixture between the Smallmouth Bass (<i>Micropterus dolomieu</i>) and the Neosho Bass (<i>M. velox</i>) in the Central Interior Highlands (CIH) ecoregion of central north America. Specifically, we used ddRADseq data to assessed the phylogenomic relationship between and within species, characterizing inter- and intraspecific diversity and SNPs potentially under local directional selection at the population level. Additionally, we inferred the relative timing of admxiture in Neosho Bass streams where there is known introgressive hybridization with Smallmouth Bass to understand the influence of natural, historic geographic factors on mixing (stream capture or transient flooding) vs. anthropogenic factors (i.e., non-native introductions through stocking), which is known to have occurred widely in these economically valuable species. We ultimately hoped to provide novel insights into the diversity of endemic, ecologically important and popular sport fish in the CIH.

## Specific Aim: Demographic analysis
In this analysis, we investigated the demographic history of populations found to be admixed between Smallmouth Bass and Neosho Bass based on admixture and phylogenomics (Analysis 3) and admixture mapping analysis (Analysis 5). Specifically, we used the joint site frequency spectrum (JSFS) of admixed populations within the Neosho Bass range (ELK, BAYOU, ILLI, and UPPARK) and the inferred interspecific parent population within the Smallmouth Bass range (SKIA, MISS, and WHITE) to determine the relative timing of admixture events by testing multiple demographic scenarios in a model-testing maximum likelihood framework. We inferred whether admixed populations were the results of relatively recent admixture, old admixture, or a combination of both and gleaned insights about the complexities of potential natural and anthropogenic sources of gene flow.

## Phases of analysis:
### Phase 1: Model identification and definitions
### Phase 2: Data preparation
### Phase 3: DADI analysis
### Phase 4:

## Libraries needed for analysis
```{r setup, echo = FALSE, include=FALSE}
library(readxl)
library(tidyverse)
library(cowplot)
library(tools)
```

## PHASE 1: MODEL IDENTIFICATION AND DEFINITIONS
In this phase of the analysis, we prepared to analyze the demographic history of admxiture events between Smallmouth Bass and Neosho Bass by selecting previously developed demographic models based on joint site frequency spectra. We reviewed the 'Two_Population_Pipeline' (Portik et al. 2017; Charles et al. 2018) for the software program DADI (Diffusion Analysis of Demographic Inference; Gutenkunst et al. 2009; see Phase 2 for citation information), which defines an array of possible models representing different demographic scenarios involving two potentially diverging and fusing populations.\

<b>Programs and sources needed:</b> <br><br>

Two_Population_Pipeline GitHub repo (Portik et al. 2017; Charles et al. 2018; retrieved from GitHub: https://github.com/dportik/dadi_pipeline)

<b>Citation</b>:<br>

Portik, D.M., Leache, A.D., Rivera, D., Blackburn, D.C., Rodel, M.-O., Barej, M.F., Hirschfeld, M., Burger, M., and M.K. Fujita. 2017. Evaluating mechanisms of diversification in a Guineo-Congolian forest frog using demographic model selection. Molecular Ecology 26: 5245-5263. doi: 10.1111/mec.14266

Charles, K.C., Bell, R.C., Blackburn, D.C., Burger, M., Fujita, M.K., Gvozdik, V., Jongsma, G.F.M., Leache, A.D., and D.M. Portik. Sky, sea, and forest islands: diversification in the African leaf-folding frog Afrixalus paradorsalis (Order: Anura, Family: Hyperoliidae). Journal of Biogeography 45: 1781-1794. doi: 10.1111/jbi.13365

### STEP 1: Populations Modeled
We modeled the demographic history of four pairs of populations, which represent admixed stream populations in the Neosho Bass native range and their corresponding Smallmouth Bass parent:

  1. Elk River (ELK) and White River (WHITE)
  2. Bayou River (BAYOU) and White River (WHITE)
  3. Upper Arkansas Tributaries (UPPARK) and the White River (WHITE)
  4. Illinois River (ILLI) and Skiatook Lake (SKIA)

### STEP 2: Models Tested
We reviewed the 'Models_2D.pdf' and chose 9 models to explicitly test for each of these four population pairs based on our knowledge of the Smallmouth Bass and Neosho system in the Central Interior Highlands. The models we tested are defined below:

  1. no_mig: Divergence with no migration
  2. sym_mig: Divdergence with continuous symmetric migration
  3. asym_mig: Divergence with continuous asymmetric migration
  4. sec_contact_sym_mig: Divergence in isolation, continuous symmetric secondary contact (recent symmetric admixture)
  5. sec_contact_asym_mig: Divergence in isolation, continuous asymmetric secondary contact (recent asymmetric admixture)
  6. anc_sym_mig: Divergence with ancient continuous symmetric migration, then isolation (ancient symmetric admixture)
  7. anc_asym_mig: Divergence with ancient continuous asymmetric migration, then isolation (ancient asymmetric admixture)
  8. sym_mig_twoepoch: Divergence with continuous symmetric migration that varies across two epochs
  9. asym_mig_twoepoch: Divergence with continuous asymmetric migration that varies across two epochs

### ----------------------- END OF PHASE 1: MODEL IDENTIFICATION AND DEFINITIONS ----------------------- ###

## PHASE 2: Data preparation
In this phase, we are preparing the fully filtered (thinned) VCF data and generating metadata population files for model testing in DADI. Since DADI assesses demographic history, we are only using neutral (non-outlier) SNP loci in all analyses. Thus, we begin with the neutral SNPs VCF generated for the species complex hierarchical level in Analysis 6 (outlier fst analysis): `outlier_fst_analysis/data/processed_vcf/02_popgen_species_complex_neutral_snps.vcf`. We further manipulate this VCF dataset into four different files, one for each pair of populations as listed above in Phase 1. 

### STEP 1: Prepare VCF input data for DADI
In this step, we are using the software VCFTools to prepare all input data for the program DADI.

#### 1a: Generate lists of samples based on population (for filtering VCF files)
In this step, we are generating text files listing samples belonging to the four two-population pairs of interest (see Phase 1 above) for DADI analysis, as inferred in Analysis 4 (population inference). These lists are then used to filter the four VCF files for downstream analyses.

##### 1a.1. Read in sample metadata and generate list of samples for the BAYOU and WHITE populations; run the Rmd chunk below:

##### Generate population file: `data/pop_files/bayou_white.txt`
```{r}
# Load in full sample metadata file with population designations (inferred in Analysis 4, population_analysis)
load("../population_analysis/data/metadata/metadata_populations_spb.Rda")

# Get only BAYOU and White populations
bayou_white <- metadata_populations_spb %>%
  filter(population == "BAYOU" | population == "WHITE")

# Select sample id column
bayou_white <- bayou_white %>%
  select(sample_id)

# Generate text file
write_tsv(bayou_white, 
          file = "data/filtering_data/bayou_white.txt", 
          col_names = FALSE)
```

##### 1a.2. Read in sample metadata and generate list of samples for the ELK and WHITE populations; run the Rmd chunk below:

##### Generate population file: `data/pop_files/elk_white.txt`
```{r}
# Get only BAYOU and White populations
elk_white <- metadata_populations_spb %>%
  filter(population == "ELK" | population == "WHITE")

# Select sample id column
elk_white <- elk_white %>%
  select(sample_id)

# Generate text file
write_tsv(elk_white, 
          file = "data/filtering_data/elk_white.txt", 
          col_names = FALSE)
```

##### 1a.3. Read in sample metadata and generate list of samples for the ILLI and SKIA populations; run the Rmd chunk below:

##### Generate population file: `data/pop_files/illi_skia.txt`
```{r}
# Get only BAYOU and White populations
illi_skia <- metadata_populations_spb %>%
  filter(population == "ILLI" | population == "SKIA")

# Select sample id column
illi_skia <- illi_skia %>%
  select(sample_id)

# Generate text file
write_tsv(illi_skia, 
          file = "data/filtering_data/illi_skia.txt", 
          col_names = FALSE)
```

##### 1a.2. Read in sample metadata and generate list of samples for the UPPARK and WHITE populations; run the Rmd chunk below:

##### Generate population file: `data/pop_files/uppark_white.txt`
```{r}
# Get only BAYOU and White populations
uppark_white <- metadata_populations_spb %>%
  filter(population == "UPPARK" | population == "WHITE")

# Select sample id column
uppark_white <- uppark_white %>%
  select(sample_id)

# Generate text file
write_tsv(uppark_white, 
          file = "data/filtering_data/uppark_white.txt", 
          col_names = FALSE)
```

#### 1b: Filter VCF data to generate datasets for each pair of populations

##### 1b.1. Generate a general bash shell script header for all VCF filtering steps commands
In this step, we are generating a universal bash shell script to run all filtering commands in VCFTOOLS (See Phase 1 in Analysis 2 for programs needed and citation information). Here, we only generate the annotated header for the shell script, and in subsequent steps, we include and provide details on each file conversion and the associated lines of code in Rmd chunks. The shell script for running all VCFTOOLS code is called "vcftools.sh".

###### 1b.1.1. Copy and paste the code below in a new shell script file:

##### Generate VCFTOOLS bash shell script: `code/shell_scripts/vcftools.sh`
```{bash}
#! /bin/bash

#SBATCH -p Lewis  # use the Lewis partition
#SBATCH -J filter_admixed  # give the job a custom name
#SBATCH -o filtering_missing.out  # give the job output a custom name
#SBATCH -t 0-02:00  # two hour time limit

#SBATCH -N 1  # number of nodes
#SBATCH -n 1  # number of cores (AKA tasks)

## UNCOMMENT LINES STARTING HERE 

## Load VCFtools module
# module load rss/rss-2020
# module load vcftools/vcftools-v0.1.14
```

##### 1b.2. Filter VCF to retain BAYOU and WHITE population samples; copy and paste the code below in the shell file generated in Step 1b.1.1 above, which will generate a filtered VCF:

##### Command line code for filtering VCF. UNCOMMENT this code in the shell script:
```{bash}
## filter 01: keep BAYOU and WHITE samples
# vcftools --vcf ../../data/processed_vcf/02_popgen_species_complex_neutral_snps.vcf --keep ../../data/filtering_data/bayou_white.txt --recode --recode-INFO-all --out ../../data/processed_vcf/01_popgen_bayou_white
```

Run: `sbatch vcftools.sh`

This code generates the file: `data/processed_vcf/01_popgen_bayou_white.vcf`.

##### 1b.3. Filter VCF to retain ELK and WHITE population samples; copy and paste the code below in the shell file generated in Step 1b.1.1 above, which will generate a filtered VCF:

##### Command line code for filtering VCF. UNCOMMENT this code in the shell script:
```{bash}
## filter 02: keep ELK and WHITE population samples
# vcftools --vcf ../../data/processed_vcf/02_popgen_species_complex_neutral_snps.vcf --keep ../../data/filtering_data/elk_white.txt --recode --recode-INFO-all --out ../../data/processed_vcf/02_popgen_elk_white
```

Run: `sbatch vcftools.sh`

This code generates the file: `data/processed_vcf/02_popgen_elk_white.vcf`.

##### 1b.4. Filter VCF to retain ILLI and SKIA population samples; copy and paste the code below in the shell file generated in Step 1b.1.1 above, which will generate a filtered VCF:

##### Command line code for filtering VCF. UNCOMMENT this code in the shell script:
```{bash}
## filter 03: keep ILLI and SKIA population samples
# vcftools --vcf ../../data/processed_vcf/02_popgen_species_complex_neutral_snps.vcf --keep ../../data/filtering_data/illi_skia.txt --recode --recode-INFO-all --out ../../data/processed_vcf/03_popgen_illi_skia
```

Run: `sbatch vcftools.sh`

This code generates the file: `data/processed_vcf/03_popgen_illi_skia.vcf`.

##### 1b.5. Filter VCF to retain UPPARK and WHITE population samples; copy and paste the code below in the shell file generated in Step 1b.1.1 above, which will generate a filtered VCF:

##### Command line code for filtering VCF. UNCOMMENT this code in the shell script:
```{bash}
## filter 04: keep UPPARK and WHITE population samples
# vcftools --vcf ../../data/processed_vcf/02_popgen_species_complex_neutral_snps.vcf --keep ../../data/filtering_data/uppark_white.txt --recode --recode-INFO-all --out ../../data/processed_vcf/04_popgen_uppark_white
```

Run: `sbatch vcftools.sh`

This code generates the file: `data/processed_vcf/04_popgen_uppark_white.vcf`.
  
### ----------------------- END OF PHASE 2: DATA PREPARATION ----------------------- ###

## PHASE 3: DADI ANALYSIS
In this phase of the analysis, we are conducting demographic analysis of admixture in our four population pairs using folded joint site frequency spectra for neutral SNP loci in teh software program DADI. Specifically, we are comparing 9 distinct, previously developed models of demographic history (see Phase 1 for models run and corresponding definitions). DADI employs maximum likelihood model testing, wherein the likelihood of a given model is determined based on residuals calculated from comparing the empirical data to theoretical data based on population genetic theory. All model likelihoods are then compared to determine the best-fitting model (the model with the smallest average residual differences between empirical and theoretical data).

DADI is written in Python and is compatible with Python Version 3. Below, we detail the steps taken to install DADI in a Python Anaconda virtual environment. We specifically used the compact version of Anaconda, Miniconda3, which only includes code for Conda, Python, and dependencies, rather than all of the software built into Anaconda (Anaconda is too large for our computing cluster disk quota). We then accessed all previously developed python scripts for running DADI on GitHub (see Phase 1 above for link to the GitHub code repository). 

All analyses conducted by calling commands in bash shell scripts were performed on the Lewis high-performance computing cluster at the University of Missouri <br>

<b>Programs needed</b>: <br><br>

DADI v.3.1.6 (Gutenkunst et al. 2009) <br>

<b>Citation</b>:<br>

Gutenkunst, R.N., Hernandez, R.D., Williamson, S.D., Bustamante, C.D. (2009). Inferring the joint demographic history of multiple populations from multidimensional SNP frequency data. PLoS Genetics 5: e1000695. doi:/10.1371/journal.pgen.1000695

### STEP 1: Load in Miniconda software and set up a Python virtual environment.
In this step, we are setting up a virtual Python working environment for running all Python analyses; we are using the software package Miniconda for Python v.3.7.

#### 1a: Generate a general bash shell script header for running all DADI commands
In this step, we are generating a universal bash shell script to run all commands in DADI. Here, we only generate the annotated header for the shell script, including loading in Miniconda3. In subsequent steps, we include and provide details on each step of the analysis and the associated lines of code in Rmd chunks. The shell script for running all DADI code is called "dadi.sh".

##### 1a.1. Copy and paste the code below in a new shell script file:

##### Generate DADI bash shell script: `code/shell_scripts/dadi.sh`
```{bash}
#! /bin/bash

#SBATCH -p Lewis  # use the Lewis partition
#SBATCH -J ILLI_SKIA  # give the job a custom name
#SBATCH -o ILLI_SKIA-%j.out  # give the job output a custom name
#SBATCH -t 2-00:00  # two day time limit
#SBATCH --mem 100G

#SBATCH -N 1  # number of nodes
#SBATCH -n 14  # number of cores (AKA tasks)

# UNCOMMENT STARTING HERE

## Commands here run only on the first core
# module load miniconda3
```

##### 1a.1. Create a Python virtual environment to run all DADI analysis, and install the DADI software library in this environment.

###### 1a.1.1. Create a new virtual environment; copy and paste the code below on the command line:

##### Command line code for generating virutal environment: `dadi_env`. UNCOMMENT this code in the shell script:
```{bash}
# conda create --name dadi_env
```

###### 1a.1.2. Activate the new conda environment; copy and past the code below on the command line:

##### Command line code for activating the `dadi_env` virtual environment. UNCOMMENT this code in the shell script:
```{bash}
# conda activate dadi_env
```

###### 1a.1.3. Install DADI; copy and past the code below on the command line:

##### Command line code for installing DADI. UNCOMMENT this code in the shell script:
```{bash}
# conda install -c conda-forge dadi
```


